{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ungraded Lab: Gradient Descent for Logistic Regression\n",
    "\n",
    "In this lab, you will implement the gradient descent update step for logistic regression.\n",
    "\n",
    "## Dataset \n",
    "Let's start with the same dataset as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll use a helper function to plot this data. The data points with label $y=1$ are shown as red crosses, while the data points with label $y=0$ are shown as black circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, '$x_1$')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEMCAYAAAAoB2Y1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOBUlEQVR4nO3db4xldX3H8c+H3TU6i4W0jnbjsjuYNBCiKdgprV1r7BIoFkL7wAfa1QdqOkljDUYbg13NzrbdNH1iJKZpMl2wGEepAUkNbVESUEpT0BlYy5/FxhAWlmB2SEPYdRsp+OmDe1eW2Z2dmZ17zrlzv+9XMtl7Z87w+10I7z3zm3PPz0kEAKjhnK4nAABoD9EHgEKIPgAUQvQBoBCiDwCFEH0AKKSV6Ns+3/Zttp+wfdD2u9oYFwDwWhtbGudGSXcleb/t10kaa2lcAMBJ3PSbs2yfJ+mApLeFd4IBQKfaWN65UNKCpC/bftj2ftubWxgXALBIG2f6k5IekLQjyYO2b5T0YpLPn3TMlKQpSdq8efNvXHzxxY3OCQBGzfz8/PNJxpc7ro3o/6qkB5JM9J//rqQbklxzuuMnJyczNzfX6JwAYNTYnk8yudxxjS/vJPmJpGdsX9T/1BWSHm96XADAqdq6eucTkmb7V+48KekjLY0LADhJK9FPckDSsj92AACaxTtyAaAQog8AhRB9ACiE6ANAIUQfAAoh+gBQCNEHgEKIPgAUQvQBoBCiDwCFEH0AKIToA0AhRB8ACiH6AFAI0QeAQog+ABRC9AGgEKIPAIUQfQAohOgDQCFEHwAKIfoAUAjRB4BCiD4AFEL0AaAQog8AhRB9AChkYxuD2H5K0lFJr0h6OclkG+MCAF6rlej3/V6S51scDwCwCMs7AFBIW9GPpO/Ynrc91dKYAIBF2lreeXeSZ22/WdLdtp9Ict+JL/b/IpiSpG3btrU0JQCop5Uz/STP9v88IukOSZcv+vpMkskkk+Pj421MCQBKajz6tjfbfuOJx5KukvRo0+MCAE7VxvLOWyTdYfvEeF9LclcL4wIAFmk8+kmelPTrTY8DAFgel2wCQCFEHwAKIfoAUAjRB4BCiD4AFEL0AaAQog8AhRB9ACiE6ANAIUQfAAoh+gBQCNEHgEKIPgAUQvQBoBCiDwCFEH0AKIToA0AhRB8ACiH6AFAI0QeAQog+ABRC9AGgEKIPAIUQfQAohOgDQCFEHwAKIfoAUAjRB4BCWou+7Q22H7Z9Z1tjAgBeq80z/eslHWxxPADAIq1E3/ZWSddI2t/GeACA02vrTP+Lkj4j6een+6LtKdtztucWFhZamhIA1NN49G1fK+lIkvmljkkyk2QyyeT4+HjTUwKAsto4098h6TrbT0m6VdJO219tYVwAwCKNRz/JZ5NsTTIh6QOS7knyoabHBQCciuv0AaCQjW0OluS7kr7b5pgAgFdxpg8AhRB9ACiE6ANAIUQfAAoh+gBQCNEHgEKIPgAUQvQBoBCiDwCFEH0AKIToA0AhRB8ACiH6AFAI0QeAQog+ABRC9AGgEKIPAIUQfQAohOgDQCFEHwAKIfoAUAjRB4BCiD4AFEL0AaAQog8AhRB9ACiE6ANAISuOvu0rbf+D7Uv7z6dW+H2vt/192z+0/ZjtvWc5VwDAGm1cxbEflfSnkj5n+5clXbrC7/uZpJ1JjtneJOl+2/+W5IHVTRUAsFarWd45muSFJH8u6SpJv7mSb0rPsf7TTf2PrG6aAIBBWE30/+XEgyQ3SPrKSr/R9gbbByQdkXR3kgcXfX3K9pztuYWFhVVMCQCwGstG3/aNtp3kn0/+fJIvrXSQJK8kuVTSVkmX2377oq/PJJlMMjk+Pr7SfywAYJVWcqZ/VNK3bI9Jku3ft/0fZzNYkhck3Svp6rP5fgDA2iz7i9wkn7P9x5K+Z/slScck3bDSAWyPS/q/JC/YfoOkKyX97dlOGABw9paNvu0rJP2JpJ9K2iLpo0l+tIoxtki6xfYG9X6y+EaSO89msgCAtVnJJZu7JX0+yf223yHpn2x/Ksk9KxkgyX9JumwtkwQADMZKlnd2nvT4Edvvk3S7pN9pcmIAgMFb9W0Ykjwn6YoG5gIAaNhZ3Xsnyf8OeiIAgOZxwzUAKIToA0AhRB8ACiH6AFAI0QeAQog+ABRC9Ns2Pd31DAAURvTbtpfdIgF0h+gDQCFEvw3T05Ld+5BefcxSD4CWORmu7WonJyczNzfX9TSaY0tD9u8cwPpnez7J5HLHcaYPAIUQ/bbt2dP1DAAURvTbxjo+gA4RfQAohOgDQCFEHwAKIfoAUAjRB4BCiD4AFEL0AaAQog8AhRB9ACiE6ANAIY1H3/YFtu+1/bjtx2xf3/SYAIDTa+NM/2VJn05yiaTflvRx25e0MC66wL2FgKHWePSTPJfkof7jo5IOSnpr0+OiI2wHCQy1Vtf0bU9IukzSg22OCwDoaS36ts+VdLukTyZ5cdHXpmzP2Z5bWFhoa0oYFLaDBNaNVrZLtL1J0p2Svp3kC2c6duS3Sxx1bAcJdGJotku0bUk3STq4XPABAM1qY3lnh6QPS9pp+0D/4w9aGBddYDtIYKhtbHqAJPdLctPjYEiwjg8MNd6RCwCFEH0AKIToA0AhRB8ACiH6AFAI0QeAQog+ABRC9AGgEKIPAIUQfQAopGz0Z2dnNTExoXPOOUcTExOanZ3tekoA0LjG770zjGZnZzU1NaXjx49Lkg4dOqSpqSlJ0q5du7qcGgA0quSZ/u7du38R/BOOHz+u3bt3dzQjAGhHyeg//fTTq/o8AIyKktHftm3bqj4PAKOiZPT37dunsbGx13xubGxM+/bt62hGANCOktHftWuXZmZmtH37dtnW9u3bNTMzwy9xAYy8VjZGXw02RgeA1RuajdEBAMOD6ANAIUQfAAoh+gBQCNEHgEKIPgAUQvQBoBCiDwCFEH0AKKTx6Nu+2fYR2482PRYA4MzaONP/R0lXtzDOUGOnLgDDoPGds5LcZ3ui6XGGGTt1ARgWrOm3gJ26AAyLoYi+7Snbc7bnFhYWup7OwLFTF4BhMRTRTzKTZDLJ5Pj4eNfTGTh26gIwLIYi+qOOnboADIs2Ltn8uqT/lHSR7cO2P9b0mMOGnboADAt2zgKAEcDOWQCAUxB9ACiE6ANAIUQfAAoh+gBQCNEHgEKIPgAUQvQBoBCiDwCFEH0AKITojyh26mrI9HTXMwDWhOiPoBM7dR06dEhJfrFTF+EfgL17u54BsCZEfwSxUxeApRD9EcROXQM2PS3ZvQ/p1ccs9WAdIvojiJ26Bmx6Wkp6H9Krj4k+1iGiP4LYqQvAUoj+CGKnrgbt2dP1DIA1YecsABgB7JwFADgF0QeAQog+ABRC9AGgEKIPAIUQfQAohOgDQCFEHwAKIfoAUAjRB4BCWom+7att/8j2j23f0MaYAIBTNR592xsk/Z2k90m6RNIHbV/S9LgAgFO1caZ/uaQfJ3kyyUuSbpX0hy2MCwBYZGMLY7xV0jMnPT8s6bdOPsD2lKSp/tOf2X60hXl15U2Snu96Eg3i9a1vo/z6Rvm1SdJFKzmojegvK8mMpBlJsj23ktuDrle8vvWN17d+jfJrk3qvbyXHtbG886ykC056vrX/OQBAy9qI/g8k/ZrtC22/TtIHJH2rhXEBAIs0vryT5GXbfybp25I2SLo5yWNn+JaZpufUMV7f+sbrW79G+bVJK3x9Q7ddIgCgObwjFwAKIfoAUMhQRX+Ub9dg+2bbR0b1PQi2L7B9r+3HbT9m+/qu5zQotl9v+/u2f9h/bXu7nlMTbG+w/bDtO7uey6DZfsr2I7YPrPTSxvXE9vm2b7P9hO2Dtt+15LHDsqbfv13Df0u6Ur03cP1A0geTPN7pxAbE9nskHZP0lSRv73o+g2Z7i6QtSR6y/UZJ85L+aBT++9m2pM1JjtneJOl+SdcneaDjqQ2U7U9JmpT0S0mu7Xo+g2T7KUmTSUbyzVm2b5H070n296+SHEvywumOHaYz/ZG+XUOS+yT9T9fzaEqS55I81H98VNJB9d6Nve6l51j/6ab+x3CcLQ2I7a2SrpG0v+u5YHVsnyfpPZJukqQkLy0VfGm4on+62zWMRDSqsT0h6TJJD3Y8lYHpL30ckHRE0t1JRua19X1R0mck/bzjeTQlkr5je75/25dRcqGkBUlf7i/P7be9eamDhyn6GAG2z5V0u6RPJnmx6/kMSpJXklyq3jvKL7c9Mkt0tq+VdCTJfNdzadC7k7xTvbv9fry/3DoqNkp6p6S/T3KZpJ9KWvJ3osMUfW7XsM7117tvlzSb5Jtdz6cJ/R+b75V0dcdTGaQdkq7rr3vfKmmn7a92O6XBSvJs/88jku5Qbzl5VByWdPiknz5vU+8vgdMapuhzu4Z1rP/LzpskHUzyha7nM0i2x22f33/8BvUuNnii00kNUJLPJtmaZEK9/+/uSfKhjqc1MLY39y8uUH/Z4ypJI3MVXZKfSHrG9om7bF4hackLKIbiLpvSWd2uYV2x/XVJ75X0JtuHJe1JclO3sxqoHZI+LOmR/tq3JP1Fkn/tbkoDs0XSLf0rzM6R9I0kI3dZ4wh7i6Q7eucl2ijpa0nu6nZKA/cJSbP9E+YnJX1kqQOH5pJNAEDzhml5BwDQMKIPAIUQfQAohOgDQCFEHwAKIfoAUAjRB86gf7voK/uP/9r2l7qeE7AWQ/PmLGBI7ZH0l7bfrN5N5K7reD7AmvDmLGAZtr8n6VxJ701y1PbbJO2WdF6S93c7O2B1WN4BzsD2O9S7DcNL/X0C1N/z4WPdzgw4O0QfWEJ/N7BZ9TbzOWZ7lO6siaKIPnAatsckfVPSp5MclPRX6q3vA+saa/rAKtn+FUn71LvF8v4kf9PxlIAVI/oAUAjLOwBQCNEHgEKIPgAUQvQBoBCiDwCFEH0AKIToA0AhRB8ACiH6AFDI/wMCz9Itm1b3nwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lab_utils import plot_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_data(X,y)\n",
    "\n",
    "# Set both axes to be from 0-6\n",
    "plt.axis([0, 6, 0, 6])\n",
    "# Set the y-axis label\n",
    "plt.ylabel('$x_2$')\n",
    "# Set the x-axis label\n",
    "plt.xlabel('$x_1$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    " First, you will implement a non-vectorized version of the gradient. Then, you will implement a vectorized version.\n",
    "\n",
    "\n",
    "### Non- vectorized version\n",
    "\n",
    "Recall that in gradient descent, each iteration performs the update:\n",
    "\n",
    "$$w_j :=w_j - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial w_j} \\tag{1}$$ \n",
    "\n",
    "simultaneously update $w_j$ for all $j$, where\n",
    "\n",
    "$$ \\frac{\\partial J(\\mathbf{w})}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 1}^{m} (f_{\\mathbf{\\mathbf{w}}}(\\mathbf{x}^{(i)}) - y^{(i)})x_j^{(i)}\\tag{2}$$ \n",
    "\n",
    "- **Note**: While this gradient looks identical to the linear regression gradient, the formula is actually different because linear and logistic regression have different definitions of $f_w(x)$.\n",
    "\n",
    "You'll implement $\\frac{\\partial J(\\mathbf{w})}{\\partial w_j}$ in this lab. \n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "    \n",
    "*  $f_{\\mathbf{w}}(x^{(i)})$ is the model's prediction, while $y^{(i)}$, which is the actual label\n",
    "\n",
    "* For a logistic regression model for the dataset given above, the model can be representented as:\n",
    "\n",
    "    $f_{\\mathbf{w}}(x) = g(w_0 + w_1x_1 + w_2x_2)$\n",
    "\n",
    "    where $g(z)$ is the sigmoid function:\n",
    "\n",
    "    $g(z) = \\frac{1}{1+e^{-z}}$ \n",
    "    \n",
    "    \n",
    "* **Preprocessing step** \n",
    "\n",
    "   For ease of implementation, we will add an additional column of ones to $X$ (as $x_0$) so that  \n",
    "    $f_{\\mathbf{w}}(x) = g(w_0x_0 + w_1x_1 + w_2x_2)$\n",
    "    \n",
    "    By doing this, to calculate the prediction from the model $f_{\\mathbf{w}}(x)$, we can write a for loop that calculates $w_jx_j$ at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.5 1.5]\n",
      " [1.  1.  1. ]\n",
      " [1.  1.5 0.5]\n",
      " [1.  3.  0.5]\n",
      " [1.  2.  2. ]\n",
      " [1.  1.  2.5]]\n"
     ]
    }
   ],
   "source": [
    "# Add a column to X_orig to account for the w_0 term\n",
    "X_mod = np.hstack([np.ones((X.shape[0],1)), X])\n",
    "\n",
    "print(X_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side Note: sigmoid function implementation\n",
    "We've implemented the `sigmoid` function for you already and you can simply import and use it, as shown in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from lab_utils import sigmoid \n",
    "\n",
    "print(sigmoid(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation\n",
    "\n",
    "Now, you'll implement the non-vectorized version of the gradient. We've already provided some starter code for you which does the following -\n",
    "* We create an array to hold the gradients (called `dw`) with the same shape as $w$ and initialize it with zeros. We will update and return `dw` \n",
    "* There is a for loop to calculate `dw[j]` at each iteration\n",
    "* At each iteration, we can use the gradient formula above, which involves another for loop over all the examples in the dataset\n",
    "* We store the gradient value for each example in a list and the gradient `dw[j]` is then computed as the sum of gradient for each example divided by the number of examples\n",
    "\n",
    "We assume that the function takes in the paramaters $w$ as a list/array.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "You'll complete the cost function by implementing the following steps inside the inner for loop - \n",
    "\n",
    "* First, you'll compute the models prediction $f_\\mathbf{w}(\\mathbf{x}^{(i)})$ for a single data point at index $i$ as shown below\n",
    "\n",
    "   ```\n",
    "   z = 0\n",
    "   for j in range(n):\n",
    "       z += w[j] * X[i][j]\n",
    "   f = sigmoid(z)\n",
    "   ```\n",
    "   \n",
    "   Since $w_0x_0 + w_1x_1+w_2x_2 = \\mathbf{w}\\cdot \\mathbf{x}$, you can also calculate  $f_\\mathbf{w}(\\mathbf{x}^{(i)})$ as \n",
    "   ```\n",
    "   z = np.dot(w, X[i])\n",
    "   f = sigmoid(z)\n",
    "   ```\n",
    "\n",
    "* Then, you'll compute the gradient for the single data point at index $i$ as \n",
    "\n",
    "  ```\n",
    "  gradient = (f-y[i])*X[i][j] \n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w):\n",
    "    # Here X is assumed to pre-processed with a column of ones added as x_0\n",
    "    m, n = X.shape\n",
    "    dw = np.zeros_like(w)\n",
    "    \n",
    "    for j in range(n):\n",
    "        gradient_list = []\n",
    "        \n",
    "        for i in range(m):        \n",
    "            ### START CODE HERE ### \n",
    "\n",
    "            ### END CODE HERE ### \n",
    "            gradient_list.append(gradient)\n",
    "        \n",
    "        dw[j] = (1/m)* sum(gradient_list)\n",
    "        \n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the implementation of your gradient function using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.25      , -0.16666667])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.zeros(3)\n",
    "compute_gradient(X_mod,y,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "array([ 0.        , -0.25      , -0.16666667])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional ) Vectorized version\n",
    "\n",
    "You will now implement a vectorized version of the gradient function.\n",
    "\n",
    "The vectorized version of the gradient formula is \n",
    "\n",
    "$$\\frac{\\partial \\mathbf{J}(w_j)}{\\partial \\mathbf{w}} = \\frac{1}{m} \\mathbf{X^T}(\\mathbf{f} - \\mathbf{y})$$ \n",
    "\n",
    "where\n",
    "\n",
    "$$ \\mathbf{f} = g(\\mathbf{X}  \\mathbf{w})$$\n",
    "\n",
    "As before, $g$ is the sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise**\n",
    "\n",
    "You'll complete the vectorized cost function by implementing the following steps - \n",
    "\n",
    "* First, you'll compute the models prediction $f(x)$ as shown below\n",
    "\n",
    "   ```\n",
    "   z = np.dot(X, w)\n",
    "   f = sigmoid(z)\n",
    "   ```\n",
    "  \n",
    "\n",
    "* Then, you'll compute the gradient as \n",
    "\n",
    "  ```\n",
    "  dw = (1/m)*np.matmul(X.T, (f - y))\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debugging Tip:** Vectorizing code can sometimes be tricky. One common strategy for debugging is to print out the sizes of the matrices you are working with using the size function. For example, given a data matrix $\\mathbf{X}$ of size 6 × 3 (6 examples, 3 features) and $\\mathbf{w}$, a vector with dimensions 3x1, you can observe that $\\mathbf{Xw}$ is a valid multiplication operation, while $\\mathbf{wX}$ is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_vectorized(X, y, w):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    \n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    ### START CODE HERE ### \n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check if the output of this function is equivalent to the output of your non-vectorized implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost computed by non-vectorized version:  [ 0.         -0.25       -0.16666667]\n",
      "Cost computed by vectorized version:  [ 0.         -0.25       -0.16666667]\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost computed by non-vectorized version: \", compute_gradient(X_mod, y, w))\n",
    "print(\"Cost computed by vectorized version: \", compute_gradient_vectorized(X_mod, y, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output** \n",
    "\n",
    "Cost computed by non-vectorized version:  [ 0.         -0.25       -0.16666667]\n",
    "\n",
    "Cost computed by vectorized version:  [ 0.         -0.25       -0.16666667]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
