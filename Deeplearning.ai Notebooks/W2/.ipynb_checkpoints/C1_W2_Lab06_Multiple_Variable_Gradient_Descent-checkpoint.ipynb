{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ungraded Lab - Multiple Variable Gradient Descent\n",
    "\n",
    "In this ungraded lab, you will extend gradient descent to support multiple features. You will utilize mean normalization and alpha tuning to improve performance. You will also utilize a popular python numeric library, NumPy to efficiently store and manipulate data. For detailed descriptions and examples of routines used, see [Numpy Documentation](https://numpy.org/doc/stable/reference/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "- [Exercise 01- Compute Gradient](#first)\n",
    "- [Exercise 02- Gradient Descent](#second)\n",
    "- [Exercise 03- Mean Normalization](#third)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Problem Statement\n",
    "\n",
    "As in the previous two labs, you will use the motivating example of housing price prediction. The training dataset contains three examples with 4 features (size,bedrooms,floors and age) shown in the table below.\n",
    "\n",
    "We would like to build a linear regression model using these values so we can then predict the price for other houses - say, a house with 1200 feet$^2$, 3 bedrooms, 1 floor, 40 years old. \n",
    "\n",
    "### 2.1 Dataset: \n",
    "| Size (feet$^2$) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  |   \n",
    "| ----------------| ------------------- |----------------- |--------------|-------------- |  \n",
    "| 2104            | 5                   | 1                | 45           | 460           |  \n",
    "| 1416            | 3                   | 2                | 40           | 232           |  \n",
    "| 852             | 2                   | 1                | 35           | 178           |  \n",
    "\n",
    "\n",
    "The lectures and equations describe $\\mathbf{X}$, $\\mathbf{y}$, $\\mathbf{w}$. In our code these are represented by variables:\n",
    "- `X_orig` represents input variables, also called input features. In previous labs, there was just one feature, now there are four. `X_train` is the data set extended with a column of ones.\n",
    "- `y_train` represents output variables, also known as target variables (in this case - Price (1000s of dollars)). \n",
    "- `w_init` represents our parameters. \n",
    "- `dw` represents our gradient. A naming convention we will use in code when referring to gradients is to infer the dJ(w) and name variables for the parameter. For example, $\\frac{\\partial J(\\mathbf{w})}{\\partial w_0}$ might be `dw0`. `dw` is the gradient vector.\n",
    "- `tmp_` is prepended to some global variable names to prevent naming conflicts.\n",
    "\n",
    "We will pick up where we left off in the last notebook. Run the following to initialize our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set\n",
    "X_orig = np.array([[2104,5,1,45], [1416,3,2,40], [852,2,1,35]])\n",
    "y_train = np.array([460,232,178]).reshape(-1,1)  #reshape creates (m,1) matrix\n",
    "\n",
    "#extend X_orig with column of ones\n",
    "tmp_ones = np.ones((3,1), dtype=np.int64)  #dtype just added to keep examples neat.. not required\n",
    "X_train = np.concatenate([tmp_ones, X_orig], axis=1)\n",
    "\n",
    "# initialize parameters to near optimal value for development\n",
    "w_init = np.array([ 785.1811367994083, 0.39133535,  18.75376741, \n",
    "                   -53.36032453, -26.42131618]).reshape(-1,1)\n",
    "print(f\"X shape: {X_train.shape}, w_shape: {w_init.shape}, y_shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Review\n",
    "In lecture, gradient descent was described as:\n",
    "\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n}\\newline & \\rbrace\\end{align*}$$\n",
    "where, parameters $w_j$ are all updated simultaniously and where  \n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial w_j}  := \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w}}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{2}\n",
    "$$\n",
    "where \n",
    "$$ f_{\\mathbf{w}}(\\mathbf{x}) =  w_0 + w_1x_1 + ... + w_nx_n \\tag{3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='first'></a>\n",
    "## Exercise 1\n",
    "We will implement a batch gradient descent algorithm for multiple variables. We'll need three functions. \n",
    "- compute_gradient implementing equation (2) above\n",
    "    - **we will do two versions** of this, one using loops, the other using linear algebra\n",
    "- compute_cost.\n",
    "- gradient_descent, utilizing compute_gradient and compute_cost, runs the iterative algorithm to find the parameters with the lowest cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_gradient using looping\n",
    "Please  extend the algorithm developed in Lab3 to support multiple variables and use NumPy. Implement equation (2) above for all $w_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='darkgreen'><b>Hints</b></font>\n",
    "</summary>\n",
    "\n",
    "```\n",
    "def compute_gradient(X, y, w): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X : (array_like Shape (m,)) variable such as house size \n",
    "      y : (array_like Shape (m,)) actual value \n",
    "      w : (array_like Shape (2,)) Initial values of parameters of the model      \n",
    "    Returns\n",
    "      dw: (array_like Shape (2,)) The gradient of the cost w.r.t. the parameters w. \n",
    "                                   Note that dw has the same dimensions as w.\n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dw = np.zeros((n,1))\n",
    "    ### START CODE HERE ### \n",
    "    for j in range(n):\n",
    "        for i in range(m):\n",
    "            f_w = 0\n",
    "            for k in range(n):\n",
    "                f_w   = f_w + w[k]*X[i][k]\n",
    "            dw[j] =  dw[j] + (f_w-y[i])*X[i][j] \n",
    "        dw[j] = dw[j]/m\n",
    "     ### END CODE HERE ###         \n",
    "        \n",
    "    return dw\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X : (array_like Shape (m,)) variable such as house size \n",
    "      y : (array_like Shape (m,)) actual value \n",
    "      w : (array_like Shape (2,)) Initial values of parameters of the model      \n",
    "    Returns\n",
    "      dw: (array_like Shape (2,)) The gradient of the cost w.r.t. the parameters w. \n",
    "                                   Note that dw has the same dimensions as w.\n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dw = np.zeros((n,1))\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    ### END CODE HERE ###         \n",
    "        \n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and display gradient \n",
    "initial_w = w_init\n",
    "grad = compute_gradient(X_train, y_train, initial_w)\n",
    "print('Gradient at initial w :\\n', grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>**Expected Output**:</b>\n",
    "</summary>\n",
    "\n",
    "```\n",
    "Gradient at initial w :\n",
    " [[-1.67392519e-06]\n",
    " [-2.72623590e-03]\n",
    " [-6.27197293e-06]\n",
    " [-2.21745582e-06]\n",
    " [-6.92403412e-05]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Gradient using Matrices\n",
    "In this section, we will implement the gradient calculation  using matrices and vectors. _If you are familiar with linear algebra, you may want to skip the explanation and try it yourself first_.\n",
    "When dealing with multi-step matrix calculations, its helpful to do 'dimensional analysis'. The diagram below details the operations involved in calculating the gradient and the dimensions of the matrices involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Gradient Matrix Calculations](./figures/Gradient.PNG \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction: $\\mathbf{f}_{\\mathbf{w}}(\\mathbf{X})$\n",
    "- This is the model's prediction for _all examples_. As in previous labs, this calculated : $\\mathbf{f}_{\\mathbf{w}}(\\mathbf{X}) = \\mathbf{X}\\mathbf{w}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_f_w = X_train @ w_init\n",
    "print(f\"The model prediction for our training set is:\")\n",
    "print(tmp_f_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error, e: $\\mathbf{f}_{\\mathbf{w}}(\\mathbf{X}) - \\mathbf{y}$\n",
    "  - This is the difference between the model prediction and the actual value of y for all training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_e = tmp_f_w - y_train\n",
    "print(\"Error\")\n",
    "print(tmp_e)\n",
    "print(f\"Error shape: {tmp_e.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient: $\\nabla_{\\mathbf{w}}\\mathbf{J}$\n",
    "-  $\\nabla_{\\mathbf{w}}\\mathbf{J}$ is the gradient of $\\mathbf{J}$ with respect to $w$ in matrix form. The upside down triagle $\\nabla$ is the symbol for graident. More simply, the result of equation 4 above for all parameters $\\mathbf{w}$\n",
    "- $\\nabla_{\\mathbf{w}}\\mathbf{J}  := \\frac{1}{m}(\\mathbf{X}^T \\mathbf{e} )$\n",
    "- Each element of this vector describes how the cost $\\mathbf{J}(\\mathbf{w})$ changes with respect to one parameter, $w_j$. For example, first element describes how the cost change relative to $w_0$. We will use this to determine if we should increase or decrease the parameter to decrease the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_m,_ = X_train.shape\n",
    "tmp_dw = (1/tmp_m) * (X_train.T @ tmp_e) \n",
    "print(\"gradient\")\n",
    "print(tmp_dw)\n",
    "print(f\"gradient shape: {tmp_dw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the equations above to implement `compute_gradient_m`, the matrix version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='darkgreen'><b>Hints</b></font>\n",
    "</summary>\n",
    "\n",
    "```\n",
    "def compute_gradient_m(X, y, w): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X : (array_like Shape (m,)) variable such as house size \n",
    "      y : (array_like Shape (m,)) actual value \n",
    "      w : (array_like Shape (2,)) Initial values of parameters of the model      \n",
    "    Returns\n",
    "      dw: (array_like Shape (2,)) The gradient of the cost w.r.t. the parameters w. \n",
    "                                   Note that dw has the same dimensions as w.\n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    ### START CODE HERE ### \n",
    "    f_w = X @ w\n",
    "    e   = f_w - y\n",
    "    dw  = (1/m) * (X.T @ e)\n",
    "    ### END CODE HERE ###         \n",
    "        \n",
    "    return dw\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_m(X, y, w): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X : (array_like Shape (m,)) variable such as house size \n",
    "      y : (array_like Shape (m,)) actual value \n",
    "      w : (array_like Shape (2,)) Initial values of parameters of the model      \n",
    "    Returns\n",
    "      dw: (array_like Shape (2,)) The gradient of the cost w.r.t. the parameters w. \n",
    "                                   Note that dw has the same dimensions as w.\n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "    ### END CODE HERE ###         \n",
    "        \n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute and display gradient USING compute_gradeint_m version\n",
    "initial_w = w_init\n",
    "grad = compute_gradient_m(X_train, y_train, initial_w)\n",
    "print('Gradient at initial w :\\n', grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>**Expected Output**:</b>\n",
    "</summary>\n",
    "\n",
    "```\n",
    "Gradient at initial w :\n",
    " [[-1.67392519e-06]\n",
    " [-2.72623590e-03]\n",
    " [-6.27197293e-06]\n",
    " [-2.21745582e-06]\n",
    " [-6.92403412e-05]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning parameters using batch gradient descent \n",
    "\n",
    "You will now find the optimal parameters of a linear regression model by implementing batch gradient descent. You can use Lab3 as a guide. \n",
    "\n",
    "- A good way to verify that gradient descent is working correctly is to look\n",
    "at the value of $J(\\mathbf{w})$ and check that it is decreasing with each step. \n",
    "\n",
    "- Assuming you have implemented the gradient and computed the cost correctly, your value of $J(\\mathbf{w})$ should never increase, and should converge to a steady value by the end of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide routine to compute cost from Lab5\n",
    "def compute_cost(X, y, w, verbose=False):\n",
    "    m,n = X.shape\n",
    "    f_w = X @ w \n",
    "    total_cost = (1/(2*m)) * np.sum((f_w-y)**2)\n",
    "    return total_cost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='second'></a>\n",
    "## Exercise 2 Implement gradient_descent:\n",
    "- Looping `num_iters` number of times\n",
    "    - calculate the gradient\n",
    "    - update the parameters using equation (1) above\n",
    "return the updated parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='darkgreen'><b>Hints</b></font>\n",
    "</summary>\n",
    "\n",
    "```\n",
    "def gradient_descent(X, y, w_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X : (array_like Shape (m,)\n",
    "      y : (array_like Shape (m,) )\n",
    "      w_in : (array_like Shape (2,)) Initial values of parameters of the model\n",
    "      cost_function: function to compute cost\n",
    "      gradient_function: function to compute the gradient\n",
    "      alpha : (float) Learning rate\n",
    "      num_iters : (int) number of iterations to run gradient descent\n",
    "    Returns\n",
    "      w : (array_like Shape (2,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "      \n",
    "       # Calculate the gradient and update the parameters\n",
    "        gradient = gradient_function(X, y, w)\n",
    "\n",
    "        # Update Parameters \n",
    "        w = w - alpha * gradient\n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost(X, y, w))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            w_history.append(w)\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, J_history, w_history #return w and J,w history for graphing\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X : (array_like Shape (m,)\n",
    "      y : (array_like Shape (m,) )\n",
    "      w_in : (array_like Shape (2,)) Initial values of parameters of the model\n",
    "      cost_function: function to compute cost\n",
    "      gradient_function: function to compute the gradient\n",
    "      alpha : (float) Learning rate\n",
    "      num_iters : (int) number of iterations to run gradient descent\n",
    "    Returns\n",
    "      w : (array_like Shape (2,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "      \n",
    "    ### START CODE HERE ### \n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "\n",
    "\n",
    "        # Update Parameters \n",
    "\n",
    "\n",
    "    ### END CODE HERE ### \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost(X, y, w))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            w_history.append(w)\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:8.2f}   \")\n",
    "        \n",
    "    return w, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we will test your implementation. Be sure to select your preferred compute_gradient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init) \n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 5.0e-7\n",
    "# run gradient descent - CHOOSE WHICH COMPUTE_GRADIENT TO RUN\n",
    "w_final, J_hist, w_hist = gradient_descent(X_train ,y_train, initial_w, compute_cost, \n",
    "                                           compute_gradient, alpha, iterations)\n",
    "#w_final, J_hist, w_hist = gradient_descent(X_train ,y_train, initial_w, compute_cost, \n",
    "#                                           compute_gradient_m, alpha, iterations)\n",
    "print(f\"w found by gradient descent: \")\n",
    "print(w_final)\n",
    "print(f\"predictions on training set\")\n",
    "print(X_train @ w_final)\n",
    "print(f\"actual values y_train \")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>**Expected Output**:</b>\n",
    "</summary>\n",
    "\n",
    " ```\n",
    "Iteration    0: Cost  2529.46   \n",
    "Iteration  100: Cost   695.99   \n",
    "Iteration  200: Cost   694.92   \n",
    "Iteration  300: Cost   693.86   \n",
    "Iteration  400: Cost   692.81   \n",
    "Iteration  500: Cost   691.77   \n",
    "Iteration  600: Cost   690.73   \n",
    "Iteration  700: Cost   689.71   \n",
    "Iteration  800: Cost   688.70   \n",
    "Iteration  900: Cost   687.69   \n",
    "w found by gradient descent: \n",
    "[[-0.00223541]\n",
    " [ 0.20396569]\n",
    " [ 0.00374919]\n",
    " [-0.0112487 ]\n",
    " [-0.0658614 ]]\n",
    "predictions on training set\n",
    "[[426.18530497]\n",
    " [286.16747201]\n",
    " [171.46763087]]\n",
    "actual values y_train \n",
    "[[460]\n",
    " [232]\n",
    " [178]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost vs iteration  \n",
    "plt.plot(J_hist)\n",
    "plt.title(\"Cost vs iteration\")\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('iteration step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*These results are not inspiring*! As in Lab 3, we have run into a situation where the mismatch in scaling between our features makes it difficult to converge. The next section will help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling or Mean Normalization\n",
    "\n",
    "We can speed up gradient descent by having each of our input values in roughly the same range. This is because the speed $\\mathbf{w}$ changes depends of the range of the input features. In our example, we have the sqft feature which is 3 orders of magnitude larger than the number of bedroom features. This doesn't allow a single alpha value to be set appropriately for all features. The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally around:    \n",
    "$$ -1 <= x_{(i)} <= 1 \\;\\;  or  \\;\\; -0.5 <= x_{(i)} <= 0.5  $$\n",
    "\n",
    "Two techniques to help with this are feature scaling and mean normalization.  \n",
    "**Feature scaling** involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1.   \n",
    "**Mean normalization** involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero.  \n",
    "In this lab we will implement _mean normalization_.\n",
    "\n",
    "To implement mean normalization, adjust your input values as shown in this formula:\n",
    "$$x_i := \\dfrac{x_i - \\mu_i}{\\sigma_i} \\tag{4}$$ \n",
    "where $i$ selects a feature or a column in our X matrix. $µ_i$ is the average of all the values for feature (i) and $\\sigma_i$ is the standard deviation over feature (i).\n",
    "\n",
    "_Usage details_: Once a model is trained with scaled features, all inputs to predictions using that model will also need to be scaled. The model targets, `y_train`, are not scaled. The resulting parameters `w` will naturally be different than those in the unscaled model.  \n",
    "Clearly you don't want to scale the $x_0$ values which we have set to one. We will scale the original data and then add a column of ones.\n",
    "\n",
    "<a name='third'></a>\n",
    "### Exercise 3 Mean Normalization\n",
    "Write a function that will accept our training data and return a mean normalized version by implementing equation (4) above. You may want to use `np.mean()`, `np.std()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <font size='3', color='darkgreen'><b>Hints</b></font>\n",
    "</summary>\n",
    "\n",
    "```\n",
    "    def mean_normalize_features(X):\n",
    "    \"\"\"\n",
    "    returns mean normalized X by column\n",
    "    Args:\n",
    "      X : (numpy array (m,n)) \n",
    "    Returns\n",
    "      X_norm: (numpy array (m,n)) input normalized by column\n",
    "    \"\"\"\n",
    "    mu     = np.mean(X,axis=0)  \n",
    "    sigma  = np.std(X,axis=0)\n",
    "    X_norm = (X - mu)/sigma      # fancy numpy broadcasting makes these look easy\n",
    "    return(X_norm)\n",
    "\n",
    "#check our work\n",
    "#from sklearn.preprocessing import scale\n",
    "#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_normalize_features(X):\n",
    "    \"\"\"\n",
    "    returns mean normalized X by column\n",
    "    Args:\n",
    "      X : (numpy array (m,n)) \n",
    "    Returns\n",
    "      X_norm: (numpy array (m,n)) input normalized by column\n",
    "    \"\"\"\n",
    "    #~ 3 lines if implemented using matrices\n",
    "    ### START CODE HERE ### \n",
    "\n",
    "   ### END CODE HERE ###         \n",
    "\n",
    "    return(X_norm)\n",
    " \n",
    "#check our work\n",
    "#from sklearn.preprocessing import scale\n",
    "#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original data:\")\n",
    "print(X_orig)\n",
    "print(\"normalized data\")\n",
    "print(mean_normalize_features(X_orig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>**Expected Output**:</b>\n",
    "</summary>\n",
    "\n",
    "```\n",
    "Original data:\n",
    "[[2104    5    1   45]\n",
    " [1416    3    2   40]\n",
    " [ 852    2    1   35]]\n",
    "normalized data\n",
    "[[ 1.26311506  1.33630621 -0.70710678  1.22474487]\n",
    " [-0.08073519 -0.26726124  1.41421356  0.        ]\n",
    " [-1.18237987 -1.06904497 -0.70710678 -1.22474487]]\n",
    "```\n",
    "Note the values in each normalized column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now normalize our original data and re-run our gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the original features\n",
    "X_norm = mean_normalize_features(X_orig)\n",
    "\n",
    "# add the column of ones and create scaled training set\n",
    "tmp_ones  = np.ones((3,1), dtype=np.int64)  #dtype just added to keep examples neat.. not required\n",
    "X_train_s = np.concatenate([tmp_ones, X_norm], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the **vastly larger value of alpha**. This will speed descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters\n",
    "initial_w = np.zeros_like(w_init) \n",
    "# some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, J_hist, w_hist = gradient_descent(X_train_s ,y_train, initial_w, \n",
    "                                           compute_cost, compute_gradient_m, alpha, iterations)\n",
    "print(f\"w found by gradient descent: \")\n",
    "print(w_final)\n",
    "print(f\"predictions on training set\")\n",
    "print(X_train_s @ w_final)\n",
    "print(f\"actual values y_train \")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "    <b>**Expected Output**:</b>\n",
    "</summary>\n",
    "    \n",
    "```\n",
    "Iteration    0: Cost 48254.77   \n",
    "Iteration  100: Cost  5582.45   \n",
    "Iteration  200: Cost   745.80   \n",
    "Iteration  300: Cost    99.90   \n",
    "Iteration  400: Cost    13.38   \n",
    "Iteration  500: Cost     1.79   \n",
    "Iteration  600: Cost     0.24   \n",
    "Iteration  700: Cost     0.03   \n",
    "Iteration  800: Cost     0.00   \n",
    "Iteration  900: Cost     0.00   \n",
    "w found by gradient descent: \n",
    "[[289.98748034]\n",
    " [ 38.05168398]\n",
    " [ 41.54320558]\n",
    " [-30.98791712]\n",
    " [ 36.34190238]]\n",
    "predictions on training set\n",
    "[[459.98690403]\n",
    " [231.98894904]\n",
    " [177.98658794]]\n",
    "actual values y_train \n",
    "[[460]\n",
    " [232]\n",
    " [178]]\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled features get very accurate results much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot cost vs iteration  \n",
    "plt.plot(J_hist)\n",
    "plt.title(\"Cost vs iteration\")\n",
    "plt.ylabel('Cost')\n",
    "plt.xlabel('iteration step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale by the learning rate: $\\alpha$\n",
    "- $\\alpha$ is a positive number smaller than 1 that reduces the magnitude of the update to be smaller than the actual gradient.\n",
    "- Try varying the learning rate in the example above. Is there a value where it diverges rather than converging?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_alpha = 0.01\n",
    "print(f\"Learning rate alpha: {tmp_alpha}\")\n",
    "\n",
    "tmp_gradient = np.array([1,2]).reshape(-1,1)\n",
    "print(\"Gradient before scaling by the learning rate:\")\n",
    "print(tmp_gradient)\n",
    "print()\n",
    "\n",
    "gradient_scaled_by_learning_rate = tmp_alpha * tmp_gradient\n",
    "print(\"Gradient after scaling by the learning rate\")\n",
    "print(gradient_scaled_by_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Subtract the gradient: $-$\n",
    "  - Recall that the gradient points in the direction that would INCREASE the cost. \n",
    "  - Negative one multiplied by the gradient will point in the direction that REDUCES the cost.\n",
    "  - So, to update the weight in the direction that reduces the cost, subtract the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_of_update = -1 * gradient_scaled_by_learning_rate\n",
    "print(\"The direction to update the parameter vector\")\n",
    "print(direction_of_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
